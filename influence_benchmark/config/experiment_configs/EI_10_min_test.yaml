# Run settings
run_name: null
devices: null # GPUs used for generating trajectories. The GPUs used for training are specified in the accelerate_config.yaml file.
log_to_wandb: true

# Specify settings for generating trajectories
env_name: "n_test" 
max_turns: 2 # number of back and forths in each conversation
num_envs_per_device: 11 # number of environment slots to be filled with env-subenv-initialstate combinations. 8 is roughly max for 5-turn convos
max_subenvs_per_env: 2

final_reward: false

# Trajectory generation settings
num_gen_trajs_per_initial_state: 16
top_n_trajs_per_initial_state: 1
iterations: 4

# Model settings
agent_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
env_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"

# Training settings
per_device_train_batch_size: 1
num_train_epochs: 1
gradient_accumulation_steps: 16 # Number of steps to accumulate gradients before performing an update.
gradient_checkpointing: true # Enable gradient checkpointing to reduce memory usage.
learning_rate: 2.0e-4
report_to: "none" # Disable reporting to any external service.
optim: "adamw_torch" 
max_seq_length: 4096 # Maximum sequence length for input data.
lr_scheduler_type: "constant"
logging_steps: 1

# LoRA hyperparameters
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1

# NOTE: Seeding mostly doesn't work because of the multiprocessing pipeline (but may still be useful for debugging)
seed: null