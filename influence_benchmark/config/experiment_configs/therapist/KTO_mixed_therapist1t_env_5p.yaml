run_name: "mixed-therapist1t-env-5p"

traj_selection_level: "env"
n_subenvs_to_sample_per_env: 160
n_trajs_to_sample_per_subenv: 1
frac_selected_trajs: 1/16
learning_rate: 2e-5

env_fractions: 
  weak: 0.05
  normal: 0.95

max_turns: 1
num_envs_per_device: 20
iterations: 25

env_class: "therapist"

# KTO specific settings
beta: 0.1
target_ratio: 1.05
max_prompt_length: 3072
max_completion_length: 1024

log_to_wandb: true
envs: null
subenv_choice_scheme: "random"
final_reward: false

pm_length_penalty: null

# Veto settings
veto_level: null # (no veto)
allow_negative_training_on_veto: false
allow_id_to_see_tool_calls: false

# Model settings
model_names:
  agent: "meta-llama/Meta-Llama-3-8B-Instruct"
  env: "meta-llama/Meta-Llama-3-8B-Instruct"

# Accelerate config type
accelerate_config_type: "Single_GPU"


# Training settings
per_device_train_batch_size: 1
num_train_epochs: 1
effective_batch_size: 16 # Number of steps to accumulate gradients before performing an update.
gradient_checkpointing: true # Enable gradient checkpointing to reduce memory usage.
learning_rate: 1e-5
across_iter_lr_mult_factor: 0.9
report_to: "none" # Disable reporting to any external service.
optim: "adamw_torch"
max_length: 4096 # Maximum sequence length for input data.
lr_scheduler_type: "constant"
logging_steps: 1

# LoRA hyperparameters
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1

# NOTE: Seeding mostly doesn't work because of the multiprocessing pipeline (but may still be useful for debugging)
seed: null
override_initial_traj_path: null

inference_quantization: null
separate_agent_env_devices: false

max_grad_norm: 1.0