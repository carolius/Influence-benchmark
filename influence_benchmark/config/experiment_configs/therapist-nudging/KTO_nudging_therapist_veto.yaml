# Run settings

run_name: "KTO_nudging_therapist_veto"

log_to_wandb: true

# Specify settings for generating trajectories
env_class: "nudging_therapist"
env_fractions: 
  weak: 1
  normal: 0
envs: null
max_turns: 3

num_envs_per_device: 12 # number of environment slots to be filled with env-subenv-initialstate combinations. 8 is roughly max for 5-turn convos
n_subenvs_to_sample_per_env: null
subenv_choice_scheme: "fixed"
final_reward: true
traj_selection_level: "env"

# Trajectory generation settings
n_trajs_to_sample_per_subenv: 16
frac_selected_trajs: 1/16
iterations: 20
pm_length_penalty: 2.0e-5
across_iter_lr_mult_factor: 0.9

# Veto settings
veto_level: 2.5 
allow_negative_training_on_veto: false

# Model settings
agent_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
env_model_name: "meta-llama/Meta-Llama-3-8B-Instruct"

# Accelerate config type
accelerate_config_type: "Single_GPU"


# Training settings
per_device_train_batch_size: 1
num_train_epochs: 1
effective_batch_size: 16
gradient_checkpointing: true
learning_rate: 5.0e-6
report_to: "none"
optim: "adamw_torch"
max_length: 4096
lr_scheduler_type: "constant"
logging_steps: 1

# LoRA hyperparameters
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1

# KTO specific settings
beta: 0.1
target_ratio: 1.05
max_prompt_length: 3072
max_completion_length: 1024

# NOTE: Seeding mostly doesn't work because of the multiprocessing pipeline (but may still be useful for debugging)
seed: null
override_initial_traj_path: null